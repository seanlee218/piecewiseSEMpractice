---
title: "Piecewise SEM Progress"
author: "Sean Lee"
date: "11/01/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Week 1

  I realized in trying to recreate Bowen et al. 2017 that I needed to actually understand what the code was actually doing.  As I was working through recreating Bowen et al. 2017's SEM model I was clueless on what each line of code was actually doing and how each component of the code ultimately fed into the final SEM model.
  When I was reseaching how to rewrite the code from Bowen et al. 2017 to work with the current version of piecewiseSEM I was brought to the github page created by the person who wrote The piecewiseSEM package, Jonathan S. Lefcheck.  

(http://jslefche.github.io/piecewiseSEM/articles/piecewiseSEM.html)

  I opened up the site and started reading the intro into SEM.  The basic concept was pretty simple in that it was a model used to establish relationships between multiple variables, through the creation of these casual networks one can establish strength and direction of direct and indirect effects of these variables.
  Whie delving headfirst into creating an SEM would have been fun, I thought I'd need some further background on how SEM's worked.  I found these videos on Youtube created by Dr. Erin M. Buchanan at Missouri State University.
  
Pt. 1
https://www.youtube.com/watch?v=2LzZvi43pGE&list=RDCMUCMdihazndR0f9XBoSXWqnYg&start_radio=1&t=87

Some basics that were mentioned in the lesson 
•	Can test multiple regressions at the same time, multiple x and y variables
•	Can test theorized causal relationships even if the researcher doesn’t initially measure the variables in a causal way
•	Error can be attributed to different parts of model rather than lumping error term together with multiple variables
Concept
	
Latent variables
•	Abstract phenomena you are trying to model in SEM (latent variables)
o	i.e. IQ, many things you can do to measure IQ but not tangible metric
•	this latent variable is linked to multiple measured variables i.e. test scores
•	it is represented indirectly by these collective variables
	
manifest or observed variables
•	measured variables, what you are actually measuring (i.e. test scores)
•	exogenous variables are variables that causal, or your independent variables
o	arrows will be going out of these variables
•	exogenous variables will not have error terms as change in these variables are represented by things you are not measuring in model (i.e. age, gender, etc.)
•	all endogenous variables have to have an error term because x(exogenous) causes y(endogenous)
o	x to predict y so there is some uncertainty about y since not getting 100% correct 
•	endogenous terms are dependent variables and arrows always leading into them
Measurement model
•	relationship between an exogenous latent variable and measured variables
•	have this latent variable think affects change ob these endogenous variables
•	i.e. we think IQ affects the SAT scores
•	these endogenous variables show what the latent variable is
•	modelling phenomena with the output of the endogenous variables
Full SEM
•	latent variable that explains latent variable that explains endogenous variable
•	variables can thus be endogenous and exogenous
•	In measurement model the 2nd circle is exogenous but in full SEM it is endogenous
•	nonRecursive model model variables can feedback on itself while the recursive the model flows in one way

hypothesis testing
•	theory building technique
•	building model to create working theory
 
•	you want data to match model
•	you want your model to be the null model
•	model things that exist rather than find significant differences to a null
•	models should explain phenomena regardless of what dataset used as the null is the norm in the world
•	 how well does our model explain the phenomena?
•	Model fit based on residuals, residuals being the error for latent variables
•	Y(persons score=data)=model(x variables)+error(residuals)
•	Since residuals are error estimated based on model it is latent (circle)
•	Low error implies data = model is more accurately modeling the phenomena you are trying to represent
•	  
•	Double error represents correlation as not sure which direction relationship is flowing

Path diagrams
 
•	X predicts y direct effect
•	X predicts z predicts y indirect effects
•	Usually would have to run 3 regressions but in SEM all in one model

Whole picture
•	 
•	Measurement model portion has one latent exogenous variable predicting the square endogenous variables and each endogenous variable gets own error term associated with it
•	Structural model has another latent exogenous variable predicting this measurement models exogenous variable that is now endogenous
•	In testing model you have to see adequacy o the model, does the x2 fit the indices?
•	In terms of theory testing does the model reflect what you hypothesized?

1. An Introduction to Structural Equation Modeling from Jonathan S. Lefcheck

  Equiped with some basic knowledge on how SEM's worked I began to work my way through the github page for piecewise SEM which is linked above.  In the example we are creating a very simple SEM using 4 variables variable x1 is exogenous to variable y1 and y2, y2 is also exogenous to y1 which is in turn exogenous to y3.
  Traditionally SEM is hard to use to analyze ecological data as the assumptions assume that data is independent and that errors are normally distributed is often not met.  
  Piecewise SEM was created and differs from traditional SEM in that each relationship between variables is estimated independently. Each local response is decomposed into individual linear or multiple regressions are used to  




```{r}

library(piecewiseSEM)
```
 
the tutorial uses this sample dataset to fit a model using piecewiseSEM.
```{r}
dat <- data.frame(x1 = runif(50), y1 = runif(50), y2 = runif(50), y3 = runif(50))
dat
```
using the "psem" function which is part of piecewiseSEM we create a series of linear regressions between the different factors.

running this code however:

model <- psem(lm(y1 ~ x1, dat), lm(y1 ~ y2, dat), lm(y2 ~ x1, dat), lm(y3 ~ y1, dat))

gives the error:

Error: Duplicate responses detected in the model list. Collapse into single multiple regression!

this is to say that the code displayed above runs all the individual component regressions which lists each "path" seperately.  psem collapses these regressions into a single multiple regression in the following format

```{r}
model <- psem(lm(y1 ~ x1 + y2, dat), lm(y2 ~ x1, dat), lm(y3 ~ y1, dat))
model
```
 to evaluate the model we call a summary of the psem object we created "model".
```{r}
summary(model, .progressBar = F)
```
# Standardization of coefficients

To compare the relative strength of the different predictors used in the model you must standardize them.  Allows for effects to be compared across multiple responses and allows for indirect and total responses to be calculated using the multiple responses.  piecewiseSEM has multiple techniques and ways to standardize coefficients and I decided the run through them all.

First the practice asked me to create another fake dataset "coefs.data"

```{r}
coefs.data <- data.frame(
  y = runif(100),
  x1 = runif(100),
  x2 = runif(100)
)

model<-lm(y~x1,coefs.data)
model
```


# No standard
 
 In some circumstances when all responses are already scaled to the same standard or in some cases you don't want your response variables to be standardized you can specify no standardization
```{r}
coefs(model, standardize = "none")

summary(model)$coefficients

## to return intercept

coefs(model, standardize = "none", intercepts = TRUE)
```
 
# standardization by standard deviation

One of the most common ways of standardization is to making the coefficients in terms of standard deviation of the mean; this is done by scaling the coefficient beta by the standard dev of x/y.

```{r}
# Obtain the raw coefficient from the coefficient table
B <- summary(model)$coefficients[2, 1]

# Compute the standard deviation of the independent variable
sd.x <- sd(coefs.data$x1)

# Compute the standard deviation of the dependent variable
sd.y <- sd(coefs.data$y)

# Scale Beta
B.sdscaled <- B * sd.x/sd.y
```

now that we got the scaled beta by hand we can compare it to the "scale" standardization option in the "coefs" function. This "scale" option standardizes by st. dev.

```{r}
coefs(model, standardize = "scale")

B.sdscaled
```
Amazing! it's a match.

# Scaling by relevant ranges

default scaling assumes you use the whole range of the dataset, but sometimes the best way to scale the data is to standardize across a relevant range

# range standardizatin by hand
```{r}
# Calculate range for the independent variable
range.x <- diff(range(coefs.data$x1))

# Calculate range for the independent variable
range.y <- diff(range(coefs.data$y))

# Scale Beta
B.range <- B * range.x/range.y
```

In "coefs" function the "range" option for the "standardize" argument standardizes the coefficients based on range difference between the variables.
```{r}
coefs(model, standardize = "range")
B.range
```
 They're the same! Amazing!
 
## GLM in pSEM

SEM shown in the example is one where x1 affects both y1 and y2 which both influence y3.  This SEM has two independence claims:

1.y3 | x1 (y1, y2)

2. y2 | y1 (x1)

Assuming we live in a gaussian world, the second independence claim should have significance values that are equal whether the test is conducted as y2 | y1 (x1) or y1 | y2 (x1).

If however we use GLM's with non-normally distributed data to generate either of the variables this would not be true.  By transforming the data the regression of y2 against y1 is going to be different than y1 regressed against y2.

The example data we generate is using a poisson distribution rather than normal to demonstrate the above statement.

```{r}
# Generate fake data
glmdat <- data.frame(x1 = runif(50), y1 = rpois(50, 10), y2 = rpois(50, 50), y3 = runif(50))

# Extract P-values
summary(lm(y1 ~ y2 + x1, glmdat))$coefficients[2, 4]
```

```{r}
summary(lm(y2 ~ y1 + x1, glmdat))$coefficients[2, 4]
```

If you are using normally distributed data then the y1 against y2 regression same as y2 against y1, amazing!

Let's try it using a poisson distribution.

```{r}
# Repeat but model y1 and y2 and Poisson-distributed
summary(glm(y1 ~ y2 + x1, "poisson", glmdat))$coefficients[2, 4]
```

```{r}
summary(glm(y2 ~ y1 + x1, "poisson", glmdat))$coefficients[2, 4]
```

when the variables y1 and y2 are poisson distributed the regression of y1 against y2 and y2 against y1 yield different significance values!

this can be problematic when performing d-seperation tests which test independence of variables not connected by an arrow while controlling variables on which these paths are conditional.  If the p-values are biased on one path vs. another then this can hinder the d-seperation test and also the goodness of fit test which can be over or underestimated based on this bias.

pSEM can solve this issue by:

1. prescribing the directionality of the tests, so you can specify y2 against y1 rather than y1 against y2

2.  One can remove one of the paths and designate it as a correlated error by using %~~%

3.  One can conduct both tests and choose the most conservative which is the one with the lowest P-value.

Let's make some fake data to demonstrate this...

```{r}
# Generate fake data
glmdat <- data.frame(x1 = runif(50), y1 = rpois(50, 10), y2 = rpois(50, 50), y3 = runif(50))

# Construct SEM
glmsem <- psem(
  glm(y1 ~ x1, "poisson", glmdat),
  glm(y2 ~ x1, "poisson", glmdat),
  lm(y3 ~ y1 + y2, glmdat)
)

#summary(glmsem)

#Error: Non-linearities detected in the basis set where P-values are not symmetrical. This can bias the outcome of the tests of directed separation. Offending independence claims: y2 <- y1 *OR* y2 -> y1 
#Option 1: Specify directionality using argument 'direction = c()' in 'summary'. 

#Option 2: Remove path from the basis set by specifying as a correlated error using '%~~%' in 'psem'. 

#Option 3 (recommended): Use argument 'conserve = TRUE' in 'summary' to compute both tests, and return the most conservative P-value.
```

#Option 1: Specify directionality using argument 'direction = c()' in 'summary'. 

In option 1 they ask to specify the directionality of the argument using the direction = c() function in the summary function

```{r}
summary(glmsem, direction = c("y1 <- y2"), .progressBar = F)$dTable

```
#Option 2: Remove path from the basis set by specifying as a correlated error using '%~~%' in 'psem'. 

In option 2 we can remove one of the paths altogether, I guess just another way of specifying the path.

```{r}
summary(update(glmsem, y1 %~~% y2), .progressBar = F)
```
Note that the claim no longer appears in the section for the tests of directed separation.

#Option 3 (recommended): Use argument 'conserve = TRUE' in 'summary' to compute both tests, and return the most conservative P-value.

we use the conserve = TRUE argument to pick the path that is most conservative "lowest p-value"

```{r}
summary(glmsem, conserve = T, .progressBar = F)$dTable
```
this seems to be y2~y1 so it chose it!

## Correlated error

correlated error is when there is no unidirectional relationship between two variables and the can affect one another bidirectionally.

the model they describe in the practice is a lot like the one above, x1 affects both y1 and y2 unidirectinally and those two variables affect y3.  But, in this example y1 and y2 are correlated bidirectionally.

in pSEM we use the %~~% argument to indicate correlated error between two different variables

```{r}
cordat <- data.frame(x1 = runif(50), y1 = runif(50), y2 = runif(50), y3 = runif(50))

corsem <- psem(
  lm(y1 ~ x1, cordat),
  lm(y2 ~ x1, cordat),
  y1 %~~% y2, 
  lm(y3 ~ y1 + y2, cordat)
)

summary(corsem, .progressBar = F)
```
In the correlated error shown above y1 and y2 are both exogenous variables and this error is calculated by performing a correlation test (cor.test) after removing the influence of the endogenous variable x1.

```{r}
cor(resid(lm(y1 ~ x1, cordat)), resid(lm(y2 ~ x1, cordat)))
```
```{r}
cerror(y1 %~~% y2, corsem)
```

## Nested models and AIC

piecewise SEM can fascilitate model comparison using comparison of model AIC scores

consider two models you are trying to compare:

SEM1 where x1 is exogenous to y1 and y2, y1 which is exogenous to y2 and y2 which is exogenous to y3

SEM2 is the same as SEM1 barring the fact that it has no y3 variable

you'd think to compare them you just use the AIC function but...

```{r}
AICdat <- data.frame(x1 = runif(50), y1 = runif(50), y2 = runif(50), y3 = runif(50))

sem1 <- psem(
  lm(y1 ~ x1, AICdat),
  lm(y2 ~ y1, AICdat),
  lm(y3 ~ y2, AICdat)
)

sem2 <- psem(
  lm(y1 ~ x1, AICdat),
  lm(y2 ~ y1, AICdat)
)

AIC(sem1, sem2)
```
The issue with this is that the 2nd sem does not account for the missing y3 varialble which is apparently important in calculating fischers C

```{r}
sem2new <- update(sem2, y3 ~ 1)

AIC(sem1, sem2new)
```
# Comparing package versions



